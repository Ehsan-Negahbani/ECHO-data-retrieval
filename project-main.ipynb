{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 1\n",
    "\n",
    "12/6/2019  \n",
    "2:30-4:50 pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the requests library \n",
    "import requests \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main:\n",
    "\n",
    "# api-endpoint \n",
    "# URL = \"http://echo.epa.gov/tools/web-services/loading-tool/dmr_rest_services.get_dmr_loadings\"\n",
    "URL = 'https://ofmpub.epa.gov/echo/dmr_rest_services.get_dmr_loadings'  \n",
    "# defining a params dict for the parameters to be sent to the API \n",
    "PARAMS = {'output':'JSON',\n",
    "         'p_year':' 2019',\n",
    "         'p_huc':'11010001',\n",
    "         'responseset': 150, # Number of rows to be retreived (#facilities*5)\n",
    "         'download':'TopFacilityPounds' # This downloads each facility 5 times (1 per top pollutant)\n",
    "         } # There might be a better API that does not do this repetititev.\n",
    "  \n",
    "# sending get request and saving the response as response object \n",
    "r = requests.get(url = URL, params = PARAMS) \n",
    "  \n",
    "# extracting data in json format \n",
    "data = r.json() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: \n",
    "\n",
    "I always set to 1000 rows, but this can be replaced by a smarter approach, where you first query the number of all facilities, then based on that and at the end double check if all of them was retreived or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Results'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data['Results']['TopFacilityPounds'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Results']['FacilityCounts']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Report:\n",
    "1. It took a lot to find the correct URL for API call. Finally I found it from https://echo.epa.gov/tools/web-services/loading-tool#/Water%20Pollution%20Search%2C%20Facility%20Reports%2C%20and%20Loading%20Calculations/get_dmr_rest_services_get_dmr_loadings. If you select the Blue \"Try it out\" button, then scroll down, where you will see the corresponding URL under \"Request URL\" section.\n",
    "2. Updated Jessie on the progress.\n",
    "3. Next steps are:\n",
    "    - work on different existing API calls to see which ones you need to extract the required information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ====================================\n",
    "Day 2\n",
    "12/12/2019 4:36 pm - 6:48 pm\n",
    "\n",
    "Goal: \n",
    "- Look for the relevant API's that provides the data required by project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facilities = data['Results']['TopFacilityPounds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "facilities_stats = data['Results']['TopFacilityPounds']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Success_flag = facility_counts = data['Results']['Message']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ExternalPermitNmbr = list()\n",
    "# FacilityName = list()\n",
    "# City = list()\n",
    "# GeocodeLatitude = list()\n",
    "# GeocodeLongitude = list()\n",
    "# MajorMinorStatusFlag = list()\n",
    "# CountyName = list()\n",
    "# SicCode1 = list()\n",
    "# PollutantDesc = list()\n",
    "data_dict = {'ExternalPermitNmbr':[],\n",
    "            'FacilityName':[],\n",
    "            'City':[],\n",
    "            'GeocodeLatitude':[],\n",
    "            'GeocodeLongitude':[],\n",
    "            'MajorMinorStatusFlag':[],\n",
    "            'CountyName':[],\n",
    "            'SicCode1':[],\n",
    "             'zip':[],\n",
    "             'FacilityType':[],\n",
    "             'PermitType':[],\n",
    "             'PermitEffectiveDate':[],\n",
    "             'PermitExpirationDate':[],\n",
    "             'ApprovedPretreatmentProgram':[],\n",
    "             'FacilityDesignFlow':[],\n",
    "             'AvgFacilityFlow':[],\n",
    "             'SicCode':[],\n",
    "             'Pollutants':[],\n",
    "             'ParameterCode':[],\n",
    "             'TotalPounds':[],\n",
    "             'MaxAllowablePounds':[],\n",
    "             'LoadOverLimit':[],\n",
    "             'TotalTwpe':[],\n",
    "             'MaxAllowableTwpe':[],\n",
    "             'LoadOverLimitTwpe':[],\n",
    "             'QcFlag':[],\n",
    "             'ExceedanceCount':[],\n",
    "             'QcReviewPounds':[],\n",
    "             'QcReviewTwpe':[]\n",
    "             \n",
    "            }\n",
    "\n",
    "facility_loading_URL = 'https://ofmpub.epa.gov/echo/dmr_rest_services.get_facility_report'  \n",
    "    # defining a params dict for the parameters to be sent to the API \n",
    "    \n",
    "facility_id_list = list() # just to check and eliminate the repeating facilities\n",
    "\n",
    "for i in range(len(facilities)):\n",
    "    if  (facilities[i]['ExternalPermitNmbr'] in facility_id_list)==False:\n",
    "        facility_id_list.append(facilities[i]['ExternalPermitNmbr'])\n",
    "        \n",
    "        PARAMS = {'output':'JSON',\n",
    "                 'p_permit_id':facilities[i]['ExternalPermitNmbr'],\n",
    "                 'p_year':'2019',\n",
    "                 } \n",
    "        status_code = 0\n",
    "        j=1\n",
    "        while status_code==0:\n",
    "            try:\n",
    "                # sending get request and saving the response as response object \n",
    "                facility_r = requests.get(url = facility_loading_URL, params = PARAMS) \n",
    "                status_code = 1\n",
    "#                 print('Succ')\n",
    "            except requests.exceptions.ConnectionError:\n",
    "        # Does it ever gets here??????\n",
    "                status_code = 0\n",
    "                print('attempt {}, connection refused!'.format(j))\n",
    "                j+=1\n",
    "                facility_r.status_code='Connection refused'\n",
    "        # extracting data in json format \n",
    "        facility_data = facility_r.json() \n",
    "        #         data_dict['Pollutants'].append(facility_data['Results']['PollutantLoads'])\n",
    "        \n",
    "        for x in facility_data['Results']['PollutantLoads']:\n",
    "            data_dict['ExternalPermitNmbr'].append(facilities[i]['ExternalPermitNmbr'])\n",
    "            data_dict['FacilityName'].append(facilities[i]['FacilityName'])\n",
    "            data_dict['City'].append(facilities[i]['City'])\n",
    "            data_dict['GeocodeLatitude'].append(facilities[i]['GeocodeLatitude'])\n",
    "            data_dict['GeocodeLongitude'].append(facilities[i]['GeocodeLongitude'])\n",
    "            data_dict['MajorMinorStatusFlag'].append(facilities[i]['MajorMinorStatusFlag'])\n",
    "            data_dict['CountyName'].append(facilities[i]['CountyName'])\n",
    "            data_dict['SicCode1'].append(facilities[i]['SicCode1'])\n",
    "            \n",
    "            data_dict['zip'].append(facility_data['Results']['FacilityInfo']['FacZip'])\n",
    "            data_dict['FacilityType'].append(facility_data['Results']['FacilityInfo']['FacilityType'])\n",
    "            data_dict['PermitType'].append(facility_data['Results']['FacilityInfo']['PermitType'])\n",
    "            data_dict['PermitEffectiveDate'].append(facility_data['Results']['FacilityInfo']['PermitEffectiveDate'])\n",
    "            data_dict['PermitExpirationDate'].append(facility_data['Results']['FacilityInfo']['PermitExpirationDate'])\n",
    "            data_dict['ApprovedPretreatmentProgram'].append(facility_data['Results']['FacilityInfo']['PretreatProgram'])\n",
    "            data_dict['FacilityDesignFlow'].append(facility_data['Results']['FacilityInfo']['FacilityDesignFlow'])\n",
    "            data_dict['AvgFacilityFlow'].append(facility_data['Results']['FacilityInfo']['AvgFacilityFlow'])\n",
    "            data_dict['SicCode'].append(facility_data['Results']['FacilityInfo']['SicCode'])\n",
    "         \n",
    "            data_dict['Pollutants'].append(x['PollutantName'])\n",
    "            data_dict['ParameterCode'].append(x['ParameterCode'])\n",
    "            data_dict['TotalPounds'].append(x['TotalPounds'])\n",
    "            data_dict['MaxAllowablePounds'].append(x['MaxAllowablePounds'])\n",
    "            data_dict['LoadOverLimit'].append(x['LoadOverLimit'])\n",
    "            data_dict['TotalTwpe'].append(x['TotalTwpe'])\n",
    "            data_dict['MaxAllowableTwpe'].append(x['MaxAllowableTwpe'])\n",
    "            data_dict['LoadOverLimitTwpe'].append(x['LoadOverLimitTwpe'])\n",
    "            data_dict['QcFlag'].append(x['QcFlag'])\n",
    "            data_dict['ExceedanceCount'].append(x['ExceedanceCount'])\n",
    "            data_dict['QcReviewPounds'].append(x['QcReviewPounds'])\n",
    "            data_dict['QcReviewTwpe'].append(x['QcReviewTwpe'])\n",
    "\n",
    "\n",
    "        print('{}/{}, {},     facility name: {}'.format(i+1, len(facilities),\n",
    "                                                        facility_data['Results']['Message'],\n",
    "                                                        facilities[i]['FacilityName']))\n",
    "        time.sleep(0.5)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Achievements at the end of day 2:\n",
    "\n",
    "\n",
    "1. Decoded the fields of json file\n",
    "2. Retrived a lot of fields of data using API.\n",
    "3. At the moment I am saving first as a dictionary, then convert it to a dataframe.\n",
    "\n",
    "\n",
    "### Things to do on day 3:\n",
    "1. Identify list of existing and missing data\n",
    "2. Work on retriving the missing data, maybe using some extra API calls about the specific polutant or the specific facility\n",
    "3. Code optimization, specially is there a better way than using dictionary+df?\n",
    "4. Populate the year and ID number columns of df.\n",
    "5. Combine the facility name with city name and figure out what is the trailing number? Is it post code?\n",
    "6. Work on the name of columns of df to match it with column names in Database Template."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 3\n",
    "12/19/2019  \n",
    "\n",
    "11:46 am - 12:40 pm\n",
    "\n",
    "4:00 pm - 7:00 pm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List of missing columns:\n",
    "- Facility type (example: POTW)\n",
    "- Permit type (example: NPDES Individual Permit)\n",
    "- Permit Effective Date\n",
    "- Permit Expiration Date\n",
    "- Approved Pretreatment Program\n",
    "- Facility Design Flow (Permit Application) (MGD)\n",
    "- Parameter code\n",
    "- MAx Allowable load\n",
    "- Max Allowable TWPE (lb-eq/yr)\n",
    "- TWPE overlimit\n",
    "- Contains potential Outliers?\n",
    "- Number of Exceedances\n",
    "\n",
    "I think majority of these missing columns relate to facility information. So lets query the information about each facility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Achievements on day 3: \n",
    "\n",
    "**Major progress**\n",
    "\n",
    "- Retreived almost all of the required fields in the proper format:\n",
    "    - There is multiple rows per facility corresponding to each pollutant\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan for Day 4:\n",
    " 1. Look again to columns of the dataframe and look for missing fields\n",
    " 2. If no missing field, then consider possible issues:\n",
    "     - I am using try-except inside a while loop to catch the unresponsiveness from server. Not sure if it catches anything at all! Make sure about this.\n",
    "     - The main (outer) query that retreives all facilities per watershed: I set the number of rows to 1000. It has a facility-stats that shows all facilities availablae in this watershed. Make sure at the end that the data for all of them are retreived.\n",
    " 3. Next step will be to add another outer call that calls all the watersheds. (So far I ahve been calling only one watershed.). This can be done with for loop that or one call with multiple watershed codes in it. Think about this, which one is better.\n",
    " 4. Finally you need to add another layer where you retreive data from different years. You can not provide multiple years within one API call. So this definitely needs separate calls for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
